{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "h4TCDtZI61jy"
   },
   "source": [
    "# COMP3222/COMP6246 Machine Learning Technologies (2019/20)\n",
    "\n",
    "## Lab 6 - Recurrent Neural Networks (Chapter 14)\n",
    "\n",
    "Follow each section at your own pace, you can have a look at the book or ask questions to demonstrators if you find something confusing.\n",
    "\n",
    "# 1. Basic Theory\n",
    "\n",
    "Until now, we looked into basic preceptrons, convolutional neural network (CNN) and how to implement them in TensorFlow. In practice these techniques are used in tasks such as: searching images, self-driving cars, automatic video classification and many more. Surely, there are different network architectures that are used in Deep Learning. In the previous lab, we showed that CNNs are essentially for `\"processing a grid of values\"`. However, the Deep Learning community has also generated another architecture specifically for `\"processing a sequence of values\"`, which are called **Recurrent Neural Networks (RNN)** [Goodfellow 2016]. In practice, recurrent neural networks are used for analyzing time series: stock prices, car trajectories, sentiment analysis and more. \n",
    "\n",
    "_Get Motivated_: Have a look at [this interactive example](https://distill.pub/2016/handwriting/), which generates new strokes in your handwriting style using RNNs. The model is explained in [this paper](https://arxiv.org/abs/1308.0850)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KGz5_b561j0"
   },
   "source": [
    "## Bare-bones RNN\n",
    "\n",
    "Let's implement an RNN with five recurrent neurons without using TensorFlow's RNN implementation/utilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PbhSJgH61j2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()\n",
    "# Let's assume some artificial data with three input (if our objective is to predict words in a sentence\n",
    "n_inputs = 3  # then for instance: first word, second word, third word can be the input of our model)\n",
    "n_neurons = 5 # number of neurons\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs]) # t=0 batch\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs]) # t=1 batch\n",
    "\n",
    "# Weights on inputs (all steps share this), initialy they are set random\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons],dtype=tf.float32))\n",
    "\n",
    "# Connection weights for the outputs of the previous timestep (all steps share this), initialy they are set random \n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32))\n",
    "\n",
    "# bias vector, all zeros for now\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# outputs of timestep 0\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "\n",
    "# outputs of timestep 1\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "# Y1 = activation_function(dot_product(Y0, Wy) + dot_product(X1, Wx) + bias_vector)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Mini-batch:        instance1  instance2   instance3 instance4\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0 (e.g. instance1=(first, second and third word))\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1 (e.g. instance1=(second, third and fourth word)\n",
    "\n",
    "# within the session\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # get the outputs of each step\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0DldZ4961j8",
    "outputId": "7751cf9a-5216-4743-a37a-037af21042a1"
   },
   "outputs": [],
   "source": [
    "print(Y0_val) # layers output at t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3Kebf4h61kE",
    "outputId": "0866a29b-b005-470c-f700-51d1dde20a81"
   },
   "outputs": [],
   "source": [
    "print(Y1_val) # layers output at t=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyfCJDzU61kL"
   },
   "source": [
    "For the given example above, from the comments in the code:\n",
    "\n",
    "*Exercise 1.2.*: How would you define the outputs?\n",
    "\n",
    "*Exercise 1.3.*: Why are there five columns?\n",
    "\n",
    "*Exercise 1.4.*: Why are there four rows?\n",
    "\n",
    "*Exercise 1.5.*: What would be the difference between `instance1` at $t=0$ and `instance1` at $t=1$?\n",
    "\n",
    "*Exercise 1.6.*: What is the difference between `instance1` and `instance2` at $t=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQRTarpY61kL"
   },
   "source": [
    "# 2. Predicting Time Series\n",
    "\n",
    "Let's look at a simple use of RNNs with time series, these time series could be stock prices, brain wave patterns and so on. Our objective could be predicting the future stock price, given the available data that we have.\n",
    "\n",
    "Let's define an arbitary sine function for stock prices `time_series(t)` to make our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpX1yze861kM",
    "outputId": "7d1fd251-d25b-420f-d19b-b795bba1203b"
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "# time starts from 0 to 30\n",
    "t_min, t_max = 0, 30\n",
    "# we sample time_series function for every 0.1 \n",
    "resolution = 0.1\n",
    "\n",
    "def time_series(t):\n",
    "    return t * np.sin(t) / 3 + 2 * np.sin(t*5)\n",
    "\n",
    "def next_batch(batch_size, n_steps):\n",
    "    \"\"\"\n",
    "    Returns a batch with `n_steps`: number of instances\n",
    "    \"\"\"\n",
    "    # randomly get a starting number between a range\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    # make a list until of number with n_steps until the next batch\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    # get the outputs of time_series functio given the input Ts (time points)\n",
    "    ys = time_series(Ts)\n",
    "    \n",
    "    # return X's and Y's\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)\n",
    "\n",
    "# inputs to the time_series function\n",
    "t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))\n",
    "\n",
    "n_steps = 20\n",
    "# a training instance\n",
    "t_instance = np.linspace(12.2, 12.2 + resolution * (n_steps + 1), n_steps + 1)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"A time series (generated)\", fontsize=14)\n",
    "# plot all the data\n",
    "plt.plot(t, time_series(t), label=r\"$t . \\sin(t) / 3 + 2 . \\sin(5t)$\")\n",
    "\n",
    "# plot only the training set\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"b-\", linewidth=3, label=\"A training instance\")\n",
    "plt.legend(loc=\"lower left\", fontsize=14)\n",
    "plt.axis([0, 30, -17, 13])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"A training instance\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "# notice that targets are shifted by one time step into the future\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"r*\", markersize=10, label=\"target\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pbmHofr61kP",
    "outputId": "99d7b4e4-8a84-44d1-cf78-e7da38f1d5ba"
   },
   "outputs": [],
   "source": [
    "X_batch, y_batch = next_batch(1, n_steps)\n",
    "\n",
    "# combining X_batch and y_batch for better printing, first_column=X, second_column=Y\n",
    "print(np.c_[X_batch[0], y_batch[0]])\n",
    "# Did you notice the shift in y values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kya6Ocrx61kS",
    "outputId": "7c26bb02-7370-4a38-bd35-2568a8cc5716"
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "learning_rate = 0.04\n",
    "n_iterations = 50\n",
    "batch_size = 50\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "\n",
    "# We use `dynamic_rnn` and `BasicRNNCell` utilities in this case, with tf.nn.relu\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "# This part is visually shown in the book Figure 14-10.\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "\n",
    "# What do you think line below will be doing? (Tip: https://www.tensorflow.org/api_docs/python/tf/layers/dense)\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})\n",
    "    saver.save(sess, \"./my_time_series_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2AYzlfn61kU",
    "outputId": "4b49110e-84e9-4ea4-a545-5057cf62c10f"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwWYJ0J561kX",
    "outputId": "be612a36-9976-4454-8da3-6ddef00e3441",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Testing the model\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"r*\", markersize=10, label=\"target\")\n",
    "plt.plot(t_instance[1:], y_pred[0,:,0], \"c.\", markersize=10, label=\"prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLMihCTA61ka"
   },
   "source": [
    "Exercise 2.1. Add comments to the code blocks above. Do you understand the purpose of each line?\n",
    "\n",
    "Exercise 2.2. How can you improve the `MSE`? _(Tip: Remember Lab 4: Gradient Descent)_\n",
    "\n",
    "Exercise 2.3. Implement the `RMSE` instead of the `MSE`, compare the test plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kCxyKLX61kb"
   },
   "source": [
    "# 3. Generative RNNs\n",
    "\n",
    "We can use RNNs to generate sequences, below you are going to use the model we trained. You should expect some resemblance to the original time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB5wmFEo61kc",
    "outputId": "1a904ae5-2b60-4b70-b014-f3e3acd0698d"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                     \n",
    "    saver.restore(sess, \"./my_time_series_model\")\n",
    "\n",
    "    sequence = [0.] * n_steps\n",
    "    for iteration in range(300):\n",
    "        X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1)\n",
    "        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "        sequence.append(y_pred[0, -1, 0])\n",
    "        \n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(np.arange(len(sequence)), sequence, \"b-\")\n",
    "plt.plot(t[:n_steps], sequence[:n_steps], \"b-\", linewidth=3)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3aWfjY961kf"
   },
   "source": [
    "Exercise 3.1. Does your plot resemble the actual time series? Why do you think so?\n",
    "\n",
    "Exercise 3.2. Change your optimizer to `AdamOptimizer`, what do you think has changed?\n",
    "\n",
    "Exercise 3.3. Try different activation functions. (e.g. logit, tanh,  ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGatAprB61kg"
   },
   "source": [
    "# Recap\n",
    "\n",
    "In this lab, we demonstrated these concepts:\n",
    "\n",
    "* from theory to implementation, how a simple RNN is works\n",
    "* how to predict a time series with RNN \n",
    "* which parameters to look out for in order to improve the predictions\n",
    "* generation of sequences with a RNN\n",
    "\n",
    "As in the previous labs, there is some material that we have not been able to cover. In your free time, you can have a look at:\n",
    "\n",
    "* LTSM Cells and GRU Cells\n",
    "* NLP Applications with RNNs\n",
    "* Encoding and Decoding with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNZMTU2261kg"
   },
   "source": [
    "### References\n",
    "[Goodfellow, 2016] : https://www.deeplearningbook.org/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
