{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"PGyBKZWSianF","colab_type":"text"},"cell_type":"markdown","source":["# COMP3222/6246 Machine Learning Technologies (2018/19)\n","## Week 8 - Introduction to Tensorflow\n","This lab is an introduction to the Tensorflow library, a powerful tool to run machine learning algorithms in Python. The Tensorflow library is the backbone of the exercises you will find in lab 5 and lab 6. Its advantages include flexibility, parallel execution, and being a general framework for computation. On top of that, it is a good entry to put in your CV!\n","## 1. Installation\n","First of all, we need to import the library in Python. Some Python distributions have it included already, if yours does not, you can sidestep the issue and use [Google's Collaboratory environment](https://colab.research.google.com/). Still, it can be a good exercise to try and install it on your local machine. In Unix system's you can simply install it by:\n","\n","```\n","pip3 install tensorflow\n","```\n","\n","After the installation, run this short test to make sure everything is working:"]},{"metadata":{"id":"tB-grTy6ianI","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","x = tf.Variable(3, name=\"x\")\n","y = tf.Variable(2, name=\"y\")\n","z = tf.Variable(1, name=\"z\")\n","g = x*y*z+x*x+z\n","\n","session = tf.Session()\n","session.run(x.initializer)\n","session.run(y.initializer)\n","session.run(z.initializer)\n","result = session.run(g)\n","session.close()\n","\n","print(result)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RbAhOboIianN","colab_type":"text"},"cell_type":"markdown","source":["The code above creates a simple function of three variables, and then runs a Tensorflow session to compute the result.\n","\n","*Exercise 1.1.* Modify the code above to compute the value of $f(x,y,z) = x^3 + y^2 + yz + 3$ with $x=-2$, $y=5$ and $z=1.2$\n","## 2. Linear regression\n","In Tensorflow, we can easily define operations on whole arrays, matrices and multi-dimensional matrices (aka tensors). In this section, we look at a straightforward implementation of the vanilla linear regression algorithm.\n","\n","Do you remember the boston house price dataset from lab 2? Let's load it again and do some regression!"]},{"metadata":{"id":"uwkIsy3FianO","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import load_boston\n","\n","# load the dataset\n","boston = load_boston()\n","m, n = boston.data.shape\n","boston_features = np.c_[np.ones((m,1)), boston.data]\n","\n","# define the pseudo-inverse equation in tensorflow\n","X = tf.constant(boston_features, dtype=tf.float32, name=\"X\")\n","y = tf.constant(boston.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n","Xt = tf.transpose(X)\n","w = tf.matmul(tf.matrix_inverse(tf.matmul(Xt, X)), y)\n","\n","# run the computation\n","with tf.Session() as sess:\n","    weights = w.eval()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YUUhdZrpianS","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 2.1.* The pseudo-inverse equation in the code above is wrong. Fix the error.\n","\n","*Exercise 2.2.* Modify the code above to compute some estimates over the training set. Print the training RMSE.\n","\n","From the examples seen so far, we can deduce that the Tensorflow library is designed around two phases. First, is the **declaration phase**, where we create all the variables and link them into a function. Internally, this generates a computation graph. Second, we create a Tensorflow session and we run the **actual computation**.\n","## 3. Gradient descent\n","When the number of features and the dataset are large, computing the pseudo-inverse can become computationally expensive. A more efficient approach is gradient descent, which consists in starting from a randomly selected point and slowly creeping toward the solution. Not only this approach is quick, but it generalises well beyond linear methods. In fact, this is the backbone of the many non-linear neural networks and deep learning algorithms that define the current state-of-the-art.\n","\n","Here is an example of how to implement gradient descent in Tensorflow. In this case, the gradients are computed automagically by automatic differentiation. This is a quite fascinating computational technique that saves us from computing first-order derivatives with pen and paper. Have a look at the Wikipedia entry to know more about this topic."]},{"metadata":{"id":"NPz5Nkc1ianU","colab_type":"code","colab":{}},"cell_type":"code","source":["n_steps = 1000\n","learn_rate = 0.001\n","\n","X = tf.constant(boston_features, dtype=tf.float32, name=\"X\")\n","y = tf.constant(boston.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n","w = tf.Variable(tf.random_uniform([n+1,1], -1.0, 1.0), name=\"w\")\n","y_hat = tf.matmul(X, w, name=\"y_hat\")\n","error = y_hat - y\n","mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n","\n","gradients = tf.gradients(mse, [w])[0]\n","train_step = tf.assign(w, w - learn_rate * gradients)\n","\n","init = tf.global_variables_initializer()\n","with tf.Session() as sess:\n","    sess.run(init)\n","    \n","    for step in range(n_steps):\n","        if step % 50 == 0:\n","            print(\"Step\", step, \"MSE =\", mse.eval())\n","        sess.run(train_step)\n","    \n","    w_best = w.eval()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RcYiUxtkianY","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 3.1.* Add comments to the code above. Do you understand the purpose of each line?\n","\n","*Exercise 3.2.* The gradient descent algorithm is really sensitive to the value of the learning rate. Try changing it by a few orders of magnitude and run the algorithm again.\n","\n","*Exercise 3.3.* Perform some feature scaling on the dataset (see lab 2), and run the gradient descent algorithm again. Do you see any difference in the result? What about the number of steps needed to converge to the optimum?\n","## 4. Principal component analysis\n","In order to improve our familiarity with Tensorflow, we play with a different topic here. One of the main problem in machine learning is how to visualise multi-dimensional data. In the case of the boston house price dataset, we have 13 input features. Can we plot this 13-dimensional space on a 2-dimensional page somehow?\n","\n","A possible solution is to use principal component analysis (PCA in short). This is an intriguing linear algebraic method that takes a cloud of multidimensional datapoints and create a new set of axes (aka components). The method extract the components that exhibit the largest variance in the data, thus spreading the datapoints as much as possible.\n","\n","In the code below, we implement PCA using Tensorflow's built-in singular value decomposition algorithm (SVD):"]},{"metadata":{"id":"F_Kxfu44ianZ","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","s, u, v = tf.svd(X)\n","P_comp = tf.slice(v, [0, 0], [n + 1, 1])\n","X_proj = tf.matmul(X, P_comp)\n","\n","with tf.Session() as sess:\n","    sess.run(X_proj)\n","    X_final = X_proj.eval()\n","\n","plt.figure()\n","plt.plot(X_final, boston.target, \".\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"53PPWu2-iane","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 4.1.* The code above plots the data along the first principal component. Modify the code to plot along the second.\n","\n","*Exercise 4.2.* Does the result change if we perform feature scaling (see lab 2) before running the PCA algorithm?"]}]}