{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFcfDOLPGbbD"
   },
   "source": [
    "# COMP3222/6246 Machine Learning Technologies (2019/20)\n",
    "# Lab 5 â€“ Perceptrons, Deep Net, and Convolutional Neural Net\n",
    "\n",
    "In this lab, we introduce how to implement a perceptron, a deep neural network and also a convolutional neural network (DNN). Though it is not a good practise, we use all the data to train and test our model for the purpose of demonstration. We also present you with a code that is working, but yields poor results. We expect you to spot these issues and improve the code. Exercises are also provided at the end of each section to improve your technical skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2o93U7kGbbG"
   },
   "source": [
    "## Setup\n",
    "\n",
    "_Make sure that the following code is executed before every other sections of this lab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLgijH5GGbbI"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxdkWQheGbbO"
   },
   "source": [
    "## A Perceptron\n",
    "\n",
    "In this section, we will use an artificial neuron (aka _perceptron_) to perform binary classification on linearly separable data. Specifically, we will use a portion of the Iris dataset; the description of this dataset can be found at <a href=\"http://scikit-learn.org/stable/datasets/index.html#iris-dataset\">http://scikit-learn.org/stable/datasets/index.html#iris-dataset</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1544279941315,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "8ILxvq1yGbbP",
    "outputId": "4eba2a05-2a7e-4f3a-d43d-662b9316fb9b"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# get dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # use only petal length and petal width\n",
    "y = (iris.target == 0).astype(np.int) # classify them as either setosa or not setosa\n",
    "\n",
    "# visualise the data\n",
    "axes = [0, 5, 0, 2]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00nQTKUyGbbY"
   },
   "source": [
    "Clearly, this task can be easily done by using a linear classifier. Could you visualise the linear decision boundary on the figure above? Where should it be?\n",
    "\n",
    "Now, let's move on to implementing a perceptron by using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1544279946077,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "SqFSWaPsGbbZ",
    "outputId": "26311b0d-7cc4-484b-88ce-951c9ee8e548"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# initialise and train a perceptron\n",
    "pct = Perceptron(max_iter=100, random_state=None)\n",
    "pct.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALYJyAB5Gbbf"
   },
   "source": [
    "Notice that there are many parameters that you can tweak later on. You can have a look at the description of each parameter in the Scikit-Learn's documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>\n",
    "\n",
    "Next, we will extract the decision boundary from the model. Below we show a general way of extracting a decision boundary with any model. Note that it can be very computationally expensive if the feature space is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1544279948705,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "-eJ4gsNBGbbg",
    "outputId": "547fcf6f-8c3c-443d-fd7d-2529f365ab35"
   },
   "outputs": [],
   "source": [
    "# sampling and predict the whole space of features\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 10).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 10).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = pct.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "# plot the datapoints again\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "# get a nice color\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "# plot the predicted samples of feature space\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PqlSqiYGbbj"
   },
   "source": [
    "**_Exercise 1_**\n",
    "1. The decision boundary of a single perceptron is a single straight line, but the above plot shows differently! Fix this plot. (_Hint_: you need to sample the feature space more)\n",
    "\n",
    "   *Solution*: Change the parameter of np.linspace above to generate more points. Have a look at the documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html.\n",
    "\n",
    "2. Try running the code in [3] and [4] multiple times; two snippets above where a network is initialised, trained, and plotted. Do you always get the same decision boundary? Why?\n",
    "\n",
    "   *Solution*: No, it's not the same boundary because we are using Stochastic Gradient Descent to train the perceptron and the random seed is also not fixed.\n",
    "\n",
    "3. A single perceptron is not different from a linear classifier, which can be described by a straight line equation. Retrieve the formula for it. Verify that this is correct by comparing it with the plot above. (_Hint_: have a look in the list of attribute on the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">online documentation</a>)\n",
    "\n",
    "   *Solution*: Basically, a perceptron multiplies each feature with weight (some value), sum them up including the bias (or the intercept), then passes to heaviside function (as specified below). Therefore, the formula can be defined accordingly as a sum of weighted inputs plus bias equating to zero. Now, you need to recover the weights from the trained perceptron; coef_ and intercept_. Then with linear algebra, you will get a straight line equation that you need to confirm with your plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il3Ml-yFGbbk"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "There are many activation functions that can be used in a perceptron. Different functions result in different behaviours, and consequently different pros & cons. Though we will not go into details, it is beneficial for you to know some popular activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDpAEXMKGbbk"
   },
   "source": [
    "$$ \\text{heaviside} (z) = \\begin{cases} 1 & \\quad \\text{if } z >= 0 \\\\ 0 & \\quad \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "$$ \\text{logit} (z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "$$ \\text{relu} (z) = \\max{\\left( 0 , z \\right)} $$\n",
    "\n",
    "$$ \\text{leaky_relu} (z, \\alpha) = \\max{\\left( \\alpha z , z \\right)} $$\n",
    "\n",
    "$$ \\text{elu} (z, \\alpha) = \\begin{cases} \\alpha \\left( e^z - 1 \\right) & \\quad \\text{if } z < 0 \\\\ z & \\quad \\text{otherwise} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcyamp_aLRuo"
   },
   "source": [
    "**_Exercise 2_** \n",
    "Complete the cell below with the code for the activation functions listed (see equations). Note that they must be able to process NumPy arrays as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Scufj5AzGbbl"
   },
   "outputs": [],
   "source": [
    "def heaviside(z): \n",
    "    return (z >= 0).astype(z.dtype)\n",
    "\n",
    "def logit(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z): \n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01): \n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "def elu(z, alpha=1): \n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "def selu(z, \n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1544125017575,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "WhQbob21Gbbq",
    "outputId": "a15fbec7-febc-4616-e4ca-bb0d0b14edf7"
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, np.tanh(z), \"b:\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, heaviside(z), \"y--\", linewidth=2, label=\"Heaviside\")\n",
    "plt.plot(z, logit(z), \"g-.\", linewidth=2, label=\"Logit\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.title(\"Activation Functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(z, relu(z), \"m-\", linewidth=2, label=\"ReLU\")\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"k:\", linewidth=2, label=\"Leaky_ReLU\")\n",
    "plt.plot(z, elu(z), \"y--\", linewidth=2, label=\"ELU\")\n",
    "plt.plot(z, selu(z), \"g-.\", linewidth=2, label=\"SELU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.title(\"Activation Functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -2, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "002ZvgQbGbbv"
   },
   "source": [
    "You should be able to see the following characteristics from the graph:\n",
    "- Step function and Heaviside function are quite similar except for their output ranges.\n",
    "- Similarly, the hyperbolic tangent and the logit/sigmoidal function are nearly the same except for their output ranges.\n",
    "- Lastly, all variants of ReLU functions behave differently only when the input sum of a perceptron is lower than zero.\n",
    "\n",
    "Note that different functions have different sensitivity to the perceptron input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmd-c30DGbbw"
   },
   "source": [
    "## Multi-Layer Perceptron (MLP) with Scikit-Learn\n",
    "\n",
    "In this section, we introduce how to implement multilayer perceptron (MLP) with Scikit-learn. Note that Scikit-learn's MLP is not suitable for very large neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRfkMnxFGbbx"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# get dataset if you haven't\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # use only petal length and petal width\n",
    "y = (iris.target == 0).astype(np.int) # classify them as either setosa or not setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1544282547230,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "EVreZXyVGbbz",
    "outputId": "0f3ac6d3-3c81-467a-f52e-a33c79312857"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialise a multi-layer perceptron\n",
    "mlp = MLPClassifier(max_iter=1, learning_rate_init=0.01, random_state=None, warm_start=True)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TNDqWWRGbb2"
   },
   "source": [
    "Note the MLP's parameters that you can play with. For a description of each parameter, have a look at the online documentation: <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html.</a>\n",
    "\n",
    "Now, we will show what the decision boundary looks like and how it changes after each training epoch. _(Note that you must generate a new MLP every time before you run the code below)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2781
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2810,
     "status": "ok",
     "timestamp": 1544282553405,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "XyjrQYZNGbb3",
    "outputId": "8d0e1746-d89c-41a3-9871-071f2faf0f42"
   },
   "outputs": [],
   "source": [
    "# Pre-define the axes for plotting\n",
    "axes = [0, 7, 0, 3]\n",
    "\n",
    "# Pre-generate a grid of sampling points\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 200).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "# Now, show the change after fitting epoch by epoch\n",
    "for epochs in range(0,100):\n",
    "    \n",
    "    # Fit the model\n",
    "    mlp.fit(X, y)\n",
    "    \n",
    "    # Plot the dataset\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "    plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "    \n",
    "    # Use to model to sampling predictions over all feature space\n",
    "    y_predict = mlp.predict(np.c_[x0.ravel(), x1.ravel()])\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "    \n",
    "    # get a nice color\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "    \n",
    "    # Use contour plot again\n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "    plt.xlabel(\"Petal length\", fontsize=14)\n",
    "    plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    plt.legend(loc=\"upper left\", fontsize=14)\n",
    "    plt.axis(axes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nr5YYkX4Gbb7"
   },
   "source": [
    "**_Exercise 3_**\n",
    "1. What is the structure of this MLP? How many neurons in each layer?\n",
    "\n",
    "   *Solution*: Since we didn't change the number of neurons in hidden layer, this MLP has 2, 100 and 1 neurons in its input layer, its hidden layer and its output layer respectively.\n",
    "\n",
    "2. Try different numbers of neurons in the hidden layer. Observe any difference during and after the training.\n",
    "\n",
    "   *Solution*: Specifying number in parameter 'hidden_layer_sizes' changes number of neurons in the hidden layer. Now, since these two categories can be linearly separated, this task is easy and can be done by a perceptron as shown earlier. Therefore, an MLP with one neuron in its hidden layer can still be used to fit these data.\n",
    "\n",
    "3. Try different activation functions such as logistic or hyperbolic tangent function. Observe any difference in the resulting plot.\n",
    "\n",
    "   *Solution*: Specifying function name in parameter 'activation' changes activation function of the hidden layer. Currently there are 4 built-in functions: *identity*, *logistic*, *tanh*, and *relu*. Since this task is easy, there is no significant difference of using different functions here. However, activation functions such as logistic and tanh should yield a training time comparably lower when you are using a very deep neural network.\n",
    "\n",
    "4. Try a stochastic gradient descent optimiser, and configure the learning rate and momentum accordingly. Observe any difference during and after the training.\n",
    "\n",
    "   *Solution*: Specifying optimiser by changing parameter *solver*. Similarly, learning rate and momentum can be configured by changing parameter *learning_rate* and *momentum* respectively. In a case where you use constant learning rate with large momentum value, your MLP's training error might not converge to a local optimum. Usually, it is therefore common to train MLP with a large number of epochs with either a small constant learning rate or a large adaptive learning rate that will decrease in each epoch.\n",
    "\n",
    "(_Hint_: Refer to the online documentation on <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "linkHUdcGbb7"
   },
   "source": [
    "## (Deeper) Neural Net for MNIST on TensorFlow\n",
    "\n",
    "In this section, we will construct and train a _deeper_ neural network with TensorFlow to perform classification. To train a large number of neurons, we would generally need a large dataset. So we will use MNIST from now on. Though it is not a good practice, we will use the whole dataset to train our neural net (for demonstration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FT_FZAsSGbb8",
    "outputId": "b0b1edc1-ea3b-495f-dc94-374f0d207afa"
   },
   "outputs": [],
   "source": [
    "# Load and use all digits in MNIST\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "digits = np.concatenate((X_train, X_test))\n",
    "labels = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Pre-processing the data\n",
    "t_digits = digits.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "t_labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "-DGiF_iXGbb_"
   },
   "source": [
    "Next, we will define a function to construct a layer of fully-connected neurons. This is more convenient than individually creating each neuron or perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_9YPoS9Gbb_"
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # Total number of pixels in each MNIST's digit\n",
    "n_hidden1 = 300 # Number of neurons in 1st hidden layer\n",
    "n_hidden2 = 100 # Number of neurons in 2nd hidden layer\n",
    "n_outputs = 10 # Number of neurons in output layer\n",
    "\n",
    "reset_graph() # as we defined in the beginning of this notebook\n",
    "\n",
    "# Create TensorFlow's placeholders for t_digits and t_labels\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# Define a function to create a layer of fully-connected neurons\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODxVRFaEGbcD"
   },
   "source": [
    "We then use this function to generate a layer of neurons that connect to either the input or the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYx4sE0hGbcF"
   },
   "outputs": [],
   "source": [
    "# Construct MLP with two layers\n",
    "hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "# Or decomment below to use TensorFlow's premade instead of our function\n",
    "#hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "#hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "#logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kvrr873qGbcI"
   },
   "source": [
    "To train this network, we need to define a loss function and choose an optimiser. After everything is constructed in TensorFlow, we run a session to execute it as usual. The code below also demonstrates how to save and restore the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "lYyRJ5-aGbcI",
    "outputId": "027f3564-5f6a-4b32-fdf1-f34286922b2b"
   },
   "outputs": [],
   "source": [
    "# Use mean softmax cross entropy as a loss function\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# Use gradient descent to train MLP\n",
    "training_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Define accuracy measure\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Initilise and run TensorFlow's computation graph of MLP\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(50):\n",
    "        sess.run(training_op, feed_dict={X: t_digits, y: t_labels})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: t_digits, y: t_labels})\n",
    "        print(epoch, \"Accuracy:\", acc_batch)\n",
    "    \n",
    "    # save the trained model\n",
    "    save_path = tf.train.Saver().save(sess, \"./trained_mnist_ann.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "KWgEK5QhGbcM",
    "outputId": "ae4ed496-944d-4a61-f47e-2a57807ea24d"
   },
   "outputs": [],
   "source": [
    "# random one digit\n",
    "rnd_id = np.random.randint(0, len(digits))\n",
    "\n",
    "# show the digit\n",
    "plt.figure()\n",
    "plt.imshow(digits[rnd_id])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "# load the trained model and use to predict\n",
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess, \"./trained_mnist_ann.ckpt\")\n",
    "    Z = logits.eval(feed_dict={X: t_digits[rnd_id].reshape(1, 28*28)})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "print(\"Predicted class: \", y_pred)\n",
    "print(\"Actual class: \", labels[rnd_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_VdtRPQGbcQ"
   },
   "source": [
    "Rerun the cell above multiple times to see how accurate our trained model is. You should be able to see that the resulted accuracy is very low and our training is slightly time-consuming.\n",
    "\n",
    "**_Exercise 4_**\n",
    "Modify and tune the neural net such that the training time is reduced but the accuracy is still acceptably high. You should try the following:\n",
    "- Change the structure of the network by adding/removing a hidden layer or increasing/reducing number of neurons.\n",
    "\n",
    "   *Solution*: You can change the structure when we used function neuron_layer above.\n",
    "\n",
    "- Change the activation function of the hidden layers.\n",
    "\n",
    "   *Solution*: Similarly, the activation function can be changed in function neuron_layer.\n",
    "\n",
    "- Choose different optimisation algorithms such as tf.train.MomentumOptimizer(), tf.train.RMSPropOptimizer, and tf.train.AdamOptimizer(). Don't forget to change the training parameters accordingly.\n",
    "\n",
    "   *Solution*: This can be changed when we defined training_op above. Details about these optimisation algorithms can be looked on https://www.tensorflow.org/api_docs/python/tf/train.\n",
    "\n",
    "Do you observe any effect on the accuracy during the tuning? What is the best model that you can achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z0GphNiGbcR"
   },
   "source": [
    "## Convolutional Neural Network (CNN) with TensorFlow\n",
    "\n",
    "We now move on to convolutional neural net (CNN). The idea behind this architecture originated from a study on the animal visual cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5FCXFfwGbcS"
   },
   "outputs": [],
   "source": [
    "# Load and use all digits in MNIST if you have directly jumped to this section\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "digits = np.concatenate((X_train, X_test))\n",
    "labels = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Pre-processing the data\n",
    "t_digits = digits.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "t_labels = labels.astype(np.int32)\n",
    "\n",
    "# MNIST's specification\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3ln9tAtGbcT"
   },
   "source": [
    "As usual, we begin with creating TensorFlow computation graph, a loss function, and an optimiser for training the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9ofQrIQGbcU"
   },
   "outputs": [],
   "source": [
    "reset_graph() # as we defined in the beginning of this notebook\n",
    "\n",
    "# Create TensorFlow's placeholders for digits and labels\n",
    "X = tf.placeholder(tf.float32, shape=[None, height * width], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "# Construct 2D convolutional layers\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=20, kernel_size=3, strides=1, \n",
    "                         padding=\"SAME\", activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=40, kernel_size=3, strides=2, \n",
    "                         padding=\"SAME\", activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# Create a max pooling layer\n",
    "pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "pool3_flat = tf.reshape(pool3, shape=[-1, 40 * 7 * 7])\n",
    "\n",
    "# Followed by layer of fully-connected neurons\n",
    "fc1 = tf.layers.dense(pool3_flat, 50, activation=tf.nn.relu, name=\"fc1\")\n",
    "logits = tf.layers.dense(fc1, 10, name=\"output\")\n",
    "\n",
    "# Use mean softmax cross entropy as a loss function\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "# Use Adam Optimiser to train CNN \n",
    "training_op = tf.train.AdamOptimizer().minimize(loss, \n",
    "                                                aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "# (Change the aggregation_method to tf.AggregationMethod.EXPERIMENTAL_TREE or DEFAULT if it doesn't work)\n",
    "\n",
    "# Define accuracy measure\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BviuGKPiGbcX"
   },
   "source": [
    "Then, we proceed to executing the TensorFlow computation graph, which will train our CNN.\n",
    "\n",
    "<font color=\"red\">**_Note that training a CNN is generally time- and memory-consuming. It is very likely that your PC will either be slow down or frozen. If this is the case, click the stop button above, wait for a couple of minutes, restart your Python kernel, and jump down to the exercise below._**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mGMd-kNGbcX"
   },
   "outputs": [],
   "source": [
    "# Define a function to make training batches\n",
    "# This is useful when your PC doesn't have much memory\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Train the CNN batch by batch\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(10):\n",
    "        for X_batch, y_batch in shuffle_batch(t_digits, t_labels, 50): \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: t_digits, y: t_labels})\n",
    "        print(epoch, \"Accuracy:\", acc_batch)\n",
    "    \n",
    "    # save the trained model\n",
    "    save_path = tf.train.Saver().save(sess, \"./trained_mnist_cnn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-plw4uePGbca"
   },
   "outputs": [],
   "source": [
    "# random one digit for test CNN's prediction\n",
    "rnd_id = np.random.randint(0, len(digits))\n",
    "\n",
    "# visualise the digit\n",
    "plt.figure()\n",
    "plt.imshow(digits[rnd_id])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "# load the trained model and use to predict\n",
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess, \"./trained_mnist_cnn.ckpt\")\n",
    "    Z = logits.eval(feed_dict={X: t_digits[rnd_id].reshape(1, 28*28)})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "\n",
    "print(\"Predicted class: \", y_pred)\n",
    "print(\"Actual class: \", labels[rnd_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eEqnbgeGbcc"
   },
   "source": [
    "**_Exercise 5_**\n",
    "1. Visualise and/or draw on your paper this convolutional neural net to figure out its current structure.\n",
    "\n",
    "   *Solution*: 1st convolutional neuron layer (20 filters, 3x3 receptive field, 1 stride, zero padding) -> 2nd convolutional neuron layer (40 filters, 3x3 receptive field, 2 strides, zero padding) -> max pooling layer (2x2 receptive field, 2 strides, no padding) -> fully-connected layer of 50 neurons -> output layer of 10 neurons\n",
    "\n",
    "2. Tune the model such that the accuracy is acceptably good, the required memory is low, and the training time is small.\n",
    "\n",
    "   *Solution*: A number of modifications can be made; e.g. removing layer, removing number of neurons, removing number of filters, increasing the size of receptive field, or increasing striding length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIzCfGQWGbcc"
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "'With 4 parameters I can fit an elephant and with 5 I can make him wiggle his trunk.' John von Neumann, _cited by Enrico Fermi in Nature 427_\n",
    "\n",
    "Do not forget that an overfitted model will not perform well in the real world. It is therefore important for you to know how to prevent this issue with neural networks in general.\n",
    "\n",
    "**_Exercise 6_**\n",
    "1. Recall the characteristic of overfitted models with respect to their performance on the training and test sets.\n",
    "\n",
    "   *Solution*: An overfitted model performs very good on the training set, but very bad on the test set.\n",
    "\n",
    "2. Restore this notebook back to its original state and then modify the code above to partition the MNIST dataset into training set and test set.\n",
    "\n",
    "   *Solution*: Dataset above has already pre-partitioned; so just removing the concatenation of training set and test set.\n",
    "\n",
    "3. Further modify the training phase of deep net and/or CNN to use only the training set and evaluate accuracy or loss on both datasets.\n",
    "\n",
    "   *Solution*: Preprocess the data for both train and test set, but assign X_train and y_train to t_digit and t_labels respectively.\n",
    "\n",
    "4. On deep net and/or CNN for MNIST above, implement one or a combination of the regularisation techniques listed below. Observe any difference or change in performance during training:\n",
    "\n",
    "   4.1. Early stopping, where you stop training your model if there is no further significant improvement of performance on your test set. (_Hint_: regularly check the performance on both sets and always store the best model)\n",
    "   \n",
    "   4.2. $l_1$ or $l_2$ regularisation, by correctly specifying TensorFlow parameters. (_Hint_: Look for 'kernel_regularizer' in the online documentation)\n",
    "   \n",
    "   4.3. Dropout, where each neuron has a probability of being turned off at each epoch in training phase (_Hint_: apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/dropout\">tf.layers.dropout()</a> to the input layer and/or any hidden layer's output, but NOT the output of the output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310338,
     "status": "ok",
     "timestamp": 1544127666064,
     "user": {
      "displayName": "Tin Leelavimolsilp",
      "photoUrl": "https://lh5.googleusercontent.com/-rO_6-RVTxhc/AAAAAAAAAAI/AAAAAAAAYzU/ZkcBNBahheI/s64/photo.jpg",
      "userId": "15221084082677372043"
     },
     "user_tz": 0
    },
    "id": "4nABRgpGHiY8",
    "outputId": "978efe46-a7b4-4d3b-8744-cf81bd2d1102"
   },
   "outputs": [],
   "source": [
    "# 4.1\n",
    "# Reload the dataset again\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "# Define and initialise functions similar to above again\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# We will need to log performace at each interval to be able to stop training early\n",
    "from datetime import datetime\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Now we start training\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_deep_mnist_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_deep_mnist_model\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7fU7EDLLDq8"
   },
   "outputs": [],
   "source": [
    "# 4.2\n",
    "# Just specify arguments kernel_regularizer when you call tf.layers.Conv2D() or tf.layers.Dense().\n",
    "# Have a look at https://keras.io/regularizers/ for 3 types of \n",
    "\n",
    "# Or you can modify the loss function to incorporate regularization; e.g. tf.nn.l2_loss()\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "regularizer = tf.nn.l2_loss(weights)\n",
    "loss = tf.reduce_mean(xentropy + beta * regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XU1XUb3WOLIf"
   },
   "outputs": [],
   "source": [
    "# 4.3\n",
    "# Examples of using tf.layers.dropout()\n",
    "hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "\n",
    "# It is similar to introduce another layers of network that will set input to zero sometime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aGfVCIzGbcc"
   },
   "source": [
    "## Sidenote\n",
    "There are many high level APIs that you can use to quickly create and deploy Machine Learning prototypes. They are very useful but it is difficult to make non-standard changes to their implementation of Machine Learning models. If you are interested, have a look on the following:\n",
    "- Estimators: <a href=\"https://www.tensorflow.org/guide/estimators\">https://www.tensorflow.org/guide/estimators</a>\n",
    "- Keras: <a href=\"https://www.tensorflow.org/guide/keras\">https://www.tensorflow.org/guide/keras</a>\n",
    "- Eager execution: <a href=\"https://www.tensorflow.org/guide/eager\">https://www.tensorflow.org/guide/eager</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic5qgx9cGbce"
   },
   "source": [
    "## Reference\n",
    "AurÃ©lien GÃ©ron, _Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab5_solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
