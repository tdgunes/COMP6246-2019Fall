{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"TQ_QOGPH-toP","colab_type":"text"},"cell_type":"markdown","source":["# COMP3222/6246 Machine Learning Technologies (2018/19)\n","## Week 4 â€“ Linear regression, polynomial regression and Support Vector Machines\n","\n","In the first lab, you implemented a simple linear regression algorithm. Despite its simplicity, this algorithm is quite powerful and can fulfill many of your ML needs. It is not surprising, then, that ML researchers have come up with all sorts of tricks to make it even more effective. In this lab, we have a look at a number of them. However, let us revise the basic model first."]},{"metadata":{"id":"gVC26ZqF-toQ","colab_type":"text"},"cell_type":"markdown","source":["## 1. Vanilla linear regression\n","For the sake of reproducibility, let us set the seed of the random generator:\n"]},{"metadata":{"id":"Ffia3FQc-toR","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","# make numpy randomisation predictable\n","np.random.seed(0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wl4lWY5D-toT","colab_type":"text"},"cell_type":"markdown","source":["Then, let us import the Boston house price dataset included in the scikit library:"]},{"metadata":{"id":"6GdZahuW-toV","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.datasets import load_boston\n","\n","# load dataset\n","boston = load_boston()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EtUslGTL-toX","colab_type":"text"},"cell_type":"markdown","source":["And split the dataset in two: 80% training set and 20% test set. It sounds familiar, doesn't it?"]},{"metadata":{"id":"QvFDgj-G-toX","colab_type":"code","colab":{}},"cell_type":"code","source":["# partition the dataset into training and test sets\n","rnd_indices = np.random.permutation(boston.data.shape[0])\n","train_size = int(boston.data.shape[0] * 0.8)\n","train_indices = rnd_indices[:train_size]\n","test_indices = rnd_indices[train_size:]\n","\n","train_data = boston.data[train_indices, :]\n","test_data = boston.data[test_indices, :]\n","train_target = boston.target[train_indices]\n","test_target = boston.target[test_indices]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZcmqrtwF-toa","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 1.1.* As a quick warm-up, modify the following code to print the fifth and sixth feature."]},{"metadata":{"id":"hjd6SJ_v-tob","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure()\n","plt.plot(train_data[:,0], train_target, \"*\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_S6CW8Dq-toc","colab_type":"text"},"cell_type":"markdown","source":["Finally, it is time to train our linear regression model:"]},{"metadata":{"id":"prDCNe0y-tod","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","# fit a linear regressor\n","lin_reg = LinearRegression()\n","lin_reg.fit(train_data, train_target)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Eq09kXCi-tof","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 1.2.* Modify the following code to print both the training and the testing RMSE."]},{"metadata":{"id":"2fIQ8TLM-tof","colab_type":"code","colab":{}},"cell_type":"code","source":["train_predict = lin_reg.predict(train_data)\n","train_rmse = np.sqrt(((train_target - train_predict) ** 2).mean())\n","print(train_rmse)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JovbOcDT-toj","colab_type":"text"},"cell_type":"markdown","source":["## 2. Feature scaling\n","The first trick we look at is a data cleaning technique, and quite a general one. Before feeding the training set to your favourite ML algorithm, it is good practice to normalise the input features. This means scaling them so that their values fall more or less in the same range. The scikit library offers two different scaling methods: min-max scaling, and standard scaling.\n","\n","*Exercise 2.1.* What do this methods do (explain in 1-2 sentences)? Hint: just google the name of the method and browse the scikit documentation!\n","\n","Let us pick the standard scaling method and write some sample code:"]},{"metadata":{"id":"OJdec8Qf-toj","colab_type":"code","colab":{}},"cell_type":"code","source":["scaler = StandardScaler()\n","scaler.fit(train_data)\n","scaled_train_data = scaler.transform(train_data)\n","scaled_test_data = scaler.transform(test_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lybzNc0R-ton","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 2.2.* Modify the code above to print the eleventh feature before and after the transformation.\n","\n","*Exercise 2.3.* Retrain the vanilla linear regressor on the scaled training set and make prediction on the (scaled) test set. Print the RMSE on training and test set."]},{"metadata":{"id":"SGs0ZDgJ-ton","colab_type":"text"},"cell_type":"markdown","source":["## 3. Regularised linear models\n","ML algorithms are affected by noise and outliers. On top of that, the model we are using might be too powerful for the problem at hand, and end up overfitting. In order to avoid this, we can constrain the result of our learning effort, and avoid choosing extreme values for the weights of our model. This is called *regularisation* and finds many applications across the ML spectrum.\n","\n","Here, we look at two alternatives for our linear model: *Ridge* regression and *Lasso* regression.\n","\n","Coding them in scikit is quite easy. Here is an example:"]},{"metadata":{"id":"1USxx1uo-too","colab_type":"code","colab":{}},"cell_type":"code","source":["# fit a ridge regressor\n","alpha_ridge = 1\n","ridge_reg = Ridge(alpha_ridge, solver=\"cholesky\")\n","ridge_reg.fit(train_data, train_target)\n","\n","# fit a lasso regressor\n","alpha_lasso = 1\n","lasso_reg = Lasso(alpha_lasso)\n","lasso_reg.fit(train_data, train_target)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l4yArNTX-toq","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 3.1.* Compare the training and test RMSE for the Ridge, Lasso and vanilla linear regressors on the Boston house price dataset (no feature scaling).\n","\n","*Exercise 3.2.* Do the results change much if we scale the input features beforehand (compare the RMSE in both cases)?\n","\n","*Exercise 3.3.* What is the best value of alpha_ridge and alpha_lasso (for simplicity use the test set as a validation set)?"]},{"metadata":{"id":"0eTtz4Cf-tor","colab_type":"text"},"cell_type":"markdown","source":["## 4. Support Vector Machines (for regression)\n","Support Vector Machines (SVM) can be used not only for classification but also for regression! Furthermore, they already provide an implicit way of regularising the result by changing the width of the margin epsilon.\n","\n","*Exercise 4.1.* Modify the code below to train a SVM regressor on the Boston house price dataset. Print the training and test RMSE."]},{"metadata":{"id":"xAvHhMir-tor","colab_type":"code","colab":{}},"cell_type":"code","source":["# fit a support vector machine regressor\n","epsilon_svm = 1\n","svm_reg = LinearSVR(epsilon_svm)\n","svm_reg.fit(train_data, train_target)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YzPQ_q_x-tot","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 4.2.* What value of epsilon gives you the best performance (for simplicity use the test set as a validation set)?"]},{"metadata":{"id":"ej7S36CM-tot","colab_type":"text"},"cell_type":"markdown","source":["## 5. Polynomial regression\n","Linear regression is all well and good, but sometimes the dataset requires a non-linear model. In this regard, the ML literature offers quite a range of non-linear regression algorithm. Here we look at the simplest one, *polynomial* regression.\n","\n","Before, implementing the algorithm, let us create a synthetic dataset:"]},{"metadata":{"id":"z2tphSGZ-tou","colab_type":"code","colab":{}},"cell_type":"code","source":["n = 100\n","data = 12 * np.random.rand(n, 1) - 3.9\n","target = 0.09 * (data**3) + 0.3 * (data**2) - 4.1 * data - 2.4 + 4.79 * np.random.randn(n, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"26EP1yHi-tow","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 5.1.* Plot the dataset.\n","\n","*Exercise 5.2.* Why would a linear model be a poor choice in this case (explain in 1-2 sentences)?\n","\n","The idea behind polynomial regression is to expand the number of input features. We do so by taking the existing ones, and multiplying them by one another. More formally, we create an arbitrary number of polynomials of the given input features.\n","\n","Fortunately, the scikit package allows us to implement this in a few lines of code:"]},{"metadata":{"id":"8BG6CElK-tox","colab_type":"code","colab":{}},"cell_type":"code","source":["# fit a quadratic regressor\n","poly_features = PolynomialFeatures(degree=2, include_bias=False)\n","poly_train_data = poly_features.fit_transform(train_data)\n","lin_reg = LinearRegression()\n","lin_reg.fit(poly_train_data, train_target)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZAiem3Er-to1","colab_type":"text"},"cell_type":"markdown","source":["The code above creates *quadratic* features, i.e. polynomials of degree two. If we need an even more powerful model, we can choose higher degrees. Of course, this will create an increasingly large number of extra features, and exposes us to overfitting. However, sometimes it is worth the effort.\n","\n","*Exercise 5.3.* Which line of the code above you need to change to implement cubic regression (report the modified line)?\n","\n","*Exercise 5.4* Train both Ridge and Lasso regressions and compare the training and test RMSE with the vanilla linear regression shown above.\n","\n","Finally, we can modify our SVM code to do polynomial regression. Notice that the SVM takes as input the regular features and manipulates them with a polynomial kernel:"]},{"metadata":{"id":"648gebOa-to3","colab_type":"code","colab":{}},"cell_type":"code","source":["svm_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n","svm_reg.fit(train_data, np.ravel(train_target))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mg5N8543-to5","colab_type":"text"},"cell_type":"markdown","source":["*Exercise 5.5* Plot the original dataset as in Exercise 5.1, but add the predictions of the four models (vanilla linear, Ridge, Lasso and SVM). Hint: you can modify the code below and print four separate plots."]},{"metadata":{"id":"Q3VI1-V1-to5","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure()\n","plt.plot(train_data, train_target, \"*\", train_data, lin_train_predict, \".\", test_data, test_target, \"*\", test_data, lin_test_predict, \".\")"],"execution_count":0,"outputs":[]}]}