{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFcfDOLPGbbD"
   },
   "source": [
    "# COMP3222/6246 Machine Learning Technologies (2019/20)\n",
    "# Week 10 – Perceptrons, Deep Net, and Convolutional Neural Net\n",
    "\n",
    "In this lab, we introduce how to implement a perceptron, a deep neural network and also a convolutional neural network (DNN). Though it is not a good practise, we use all the data to train and test our model for the purpose of demonstration. We also present you with a code that is working, but yields poor results. We expect you to spot these issues and improve the code. Exercises are also provided at the end of each section to improve your technical skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2o93U7kGbbG"
   },
   "source": [
    "## Setup\n",
    "\n",
    "_Make sure that the following code is executed before every other sections of this lab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLgijH5GGbbI"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxdkWQheGbbO"
   },
   "source": [
    "## A Perceptron\n",
    "\n",
    "In this section, we will use an artificial neuron (aka _perceptron_) to perform binary classification on linearly separable data. Specifically, we will use a portion of the Iris dataset; the description of this dataset can be found at <a href=\"http://scikit-learn.org/stable/datasets/index.html#iris-dataset\">http://scikit-learn.org/stable/datasets/index.html#iris-dataset</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "8ILxvq1yGbbP",
    "outputId": "0a7bdc4d-1a1f-4445-e2f2-405e3a792ab0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# get dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # use only petal length and petal width\n",
    "y = (iris.target == 0).astype(np.int) # classify them as either setosa or not setosa\n",
    "\n",
    "# visualise the data\n",
    "axes = [0, 5, 0, 2]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00nQTKUyGbbY"
   },
   "source": [
    "Clearly, this task can be easily done by using a linear classifier. Could you visualise the linear decision boundary on the figure above? Where should it be?\n",
    "\n",
    "Now, let's move on to implementing a perceptron by using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SqFSWaPsGbbZ",
    "outputId": "4bf51d93-25d1-48d3-fd07-60d30627bc48"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# initialise and train a perceptron\n",
    "pct = Perceptron(max_iter=100, random_state=None)\n",
    "pct.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALYJyAB5Gbbf"
   },
   "source": [
    "Notice that there are many parameters that you can tweak later on. You can have a look at the description of each parameter in the Scikit-Learn's documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>\n",
    "\n",
    "Next, we will extract the decision boundary from the model. Below we show a general way of extracting a decision boundary with any model. Note that it can be very computationally expensive if the feature space is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "-eJ4gsNBGbbg",
    "outputId": "7873dd0a-db0b-4eed-8180-faa4f30cbadb"
   },
   "outputs": [],
   "source": [
    "# sampling and predict the whole space of features\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 10).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 10).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = pct.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "# plot the datapoints again\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "# get a nice color\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "# plot the predicted samples of feature space\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PqlSqiYGbbj"
   },
   "source": [
    "**_Exercise 1_**\n",
    "1. The decision boundary of a single perceptron is a single straight line, but the above plot shows differently! Fix this plot. (_Hint_: you need to sample the feature space more)\n",
    "2. Try running the code in [3] and [4] multiple times; two snippets above where a network is initialised, trained, and plotted. Do you always get the same decision boundary? Why?\n",
    "3. A single perceptron is not different from a linear classifier, which can be described by a straight line equation. Retrieve the formula for it. Verify that this is correct by comparing it with the plot above. (_Hint_: have a look in the list of attribute on the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">online documentation</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il3Ml-yFGbbk"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "There are many activation functions that can be used in a perceptron. Different functions result in different behaviours, and consequently different pros & cons. Though we will not go into details, it is beneficial for you to know some popular activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDpAEXMKGbbk"
   },
   "source": [
    "$$ \\text{heaviside} (z) = \\begin{cases} 1 & \\quad \\text{if } z >= 0 \\\\ 0 & \\quad \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "$$ \\text{logit} (z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "$$ \\text{relu} (z) = \\max{\\left( 0 , z \\right)} $$\n",
    "\n",
    "$$ \\text{leaky_relu} (z, \\alpha) = \\max{\\left( \\alpha z , z \\right)} $$\n",
    "\n",
    "$$ \\text{elu} (z, \\alpha) = \\begin{cases} \\alpha \\left( e^z - 1 \\right) & \\quad \\text{if } z < 0 \\\\ z & \\quad \\text{otherwise} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcyamp_aLRuo"
   },
   "source": [
    "**_Exercise 2_** \n",
    "Complete the cell below with the code for the activation functions listed (see equations). Note that they must be able to process NumPy arrays as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Scufj5AzGbbl"
   },
   "outputs": [],
   "source": [
    "def heaviside(z): # modify this function. Hint: Use astype(z.dtype)\n",
    "    return 0\n",
    "\n",
    "def logit(z): # modify this function. Hint: Use np.exp()\n",
    "    return 0\n",
    "\n",
    "def relu(z): # modify this function. Hint: Use np.maximum()\n",
    "    return 0\n",
    "\n",
    "def leaky_relu(z, alpha): # modify this function and set default alpha to 0.01\n",
    "    return 0\n",
    "\n",
    "def elu(z, alpha=1): # No need to modify this function!\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "def selu(z, # No need to modify this function!\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "WhQbob21Gbbq",
    "outputId": "d3636e27-5780-45af-8cde-ce27eeccc579"
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, np.tanh(z), \"b:\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, heaviside(z), \"y--\", linewidth=2, label=\"Heaviside\")\n",
    "plt.plot(z, logit(z), \"g-.\", linewidth=2, label=\"Logit\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.title(\"Activation Functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(z, relu(z), \"m-\", linewidth=2, label=\"ReLU\")\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"k:\", linewidth=2, label=\"Leaky_ReLU\")\n",
    "plt.plot(z, elu(z), \"y--\", linewidth=2, label=\"ELU\")\n",
    "plt.plot(z, selu(z), \"g-.\", linewidth=2, label=\"SELU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.title(\"Activation Functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -2, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "002ZvgQbGbbv"
   },
   "source": [
    "You should be able to see the following characteristics from the graph:\n",
    "- Step function and Heaviside function are quite similar except for their output ranges.\n",
    "- Similarly, the hyperbolic tangent and the logit/sigmoidal function are nearly the same except for their output ranges.\n",
    "- Lastly, all variants of ReLU functions behave differently only when the input sum of a perceptron is lower than zero.\n",
    "\n",
    "Note that different functions have different sensitivity to the perceptron input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmd-c30DGbbw"
   },
   "source": [
    "## Multi-Layer Perceptron (MLP) with Scikit-Learn\n",
    "\n",
    "In this section, we introduce how to implement multilayer perceptron (MLP) with Scikit-learn. Note that Scikit-learn's MLP is not suitable for very large neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRfkMnxFGbbx"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# get dataset if you haven't\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # use only petal length and petal width\n",
    "y = (iris.target == 0).astype(np.int) # classify them as either setosa or not setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "EVreZXyVGbbz",
    "outputId": "f911e03e-fda6-4887-cd6a-db8000234833"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialise a multi-layer perceptron\n",
    "mlp = MLPClassifier(max_iter=1, learning_rate_init=0.01, random_state=None, warm_start=True)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TNDqWWRGbb2"
   },
   "source": [
    "Note the MLP's parameters that you can play with. For a description of each parameter, have a look at the online documentation: <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html.</a>\n",
    "\n",
    "Now, we will show what the decision boundary looks like and how it changes after each training epoch. _(Note that you must generate a new MLP every time before you run the code below)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8241
    },
    "colab_type": "code",
    "id": "XyjrQYZNGbb3",
    "outputId": "c698e36b-84fa-44ff-c06f-2247bf56039e"
   },
   "outputs": [],
   "source": [
    "# Pre-define the axes for plotting\n",
    "axes = [0, 7, 0, 3]\n",
    "\n",
    "# Pre-generate a grid of sampling points\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 200).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "# Now, show the change after fitting epoch by epoch\n",
    "for epochs in range(0,30):\n",
    "    \n",
    "    # Fit the model\n",
    "    mlp.fit(X, y)\n",
    "    \n",
    "    # Plot the dataset\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "    plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "    \n",
    "    # Use to model to sampling predictions over all feature space\n",
    "    y_predict = mlp.predict(np.c_[x0.ravel(), x1.ravel()])\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "    \n",
    "    # get a nice color\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "    \n",
    "    # Use contour plot again\n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "    plt.xlabel(\"Petal length\", fontsize=14)\n",
    "    plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    plt.legend(loc=\"upper left\", fontsize=14)\n",
    "    plt.axis(axes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nr5YYkX4Gbb7"
   },
   "source": [
    "**_Exercise 3_**\n",
    "1. What is the structure of this MLP? How many neurons in each layer?\n",
    "2. Try different numbers of neurons in the hidden layer. Observe any difference during and after the training.\n",
    "3. Try different activation functions such as logistic or hyperbolic tangent function. Observe any difference in the resulting plot.\n",
    "4. Try a stochastic gradient descent optimiser, and configure the learning rate and momentum accordingly. Observe any difference during and after the training.\n",
    "\n",
    "(_Hint_: Refer to the online documentation on <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "linkHUdcGbb7"
   },
   "source": [
    "## (Deeper) Neural Net for MNIST on TensorFlow\n",
    "\n",
    "In this section, we will construct and train a _deeper_ neural network with TensorFlow to perform classification. To train a large number of neurons, we would generally need a large dataset. So we will use MNIST from now on. Though it is not a good practice, we will use the whole dataset to train our neural net (for demonstration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FT_FZAsSGbb8",
    "outputId": "b0b1edc1-ea3b-495f-dc94-374f0d207afa"
   },
   "outputs": [],
   "source": [
    "# Load and use all digits in MNIST\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "digits = np.concatenate((X_train, X_test))\n",
    "labels = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Pre-processing the data\n",
    "t_digits = digits.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "t_labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "-DGiF_iXGbb_"
   },
   "source": [
    "Next, we will define a function to construct a layer of fully-connected neurons. This is more convenient than individually creating each neuron or perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_9YPoS9Gbb_"
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # Total number of pixels in each MNIST's digit\n",
    "n_hidden1 = 300 # Number of neurons in 1st hidden layer\n",
    "n_hidden2 = 100 # Number of neurons in 2nd hidden layer\n",
    "n_outputs = 10 # Number of neurons in output layer\n",
    "\n",
    "reset_graph() # as we defined in the beginning of this notebook\n",
    "\n",
    "# Create TensorFlow's placeholders for t_digits and t_labels\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# Define a function to create a layer of fully-connected neurons\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODxVRFaEGbcD"
   },
   "source": [
    "We then use this function to generate a layer of neurons that connect to either the input or the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYx4sE0hGbcF"
   },
   "outputs": [],
   "source": [
    "# Construct MLP with two layers\n",
    "hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "# Or decomment below to use TensorFlow's premade instead of our function\n",
    "#hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "#hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "#logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kvrr873qGbcI"
   },
   "source": [
    "To train this network, we need to define a loss function and choose an optimiser. After everything is constructed in TensorFlow, we run a session to execute it as usual. The code below also demonstrates how to save and restore the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "lYyRJ5-aGbcI",
    "outputId": "027f3564-5f6a-4b32-fdf1-f34286922b2b"
   },
   "outputs": [],
   "source": [
    "# Use mean softmax cross entropy as a loss function\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# Use gradient descent to train MLP\n",
    "training_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Define accuracy measure\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Initilise and run TensorFlow's computation graph of MLP\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(50):\n",
    "        sess.run(training_op, feed_dict={X: t_digits, y: t_labels})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: t_digits, y: t_labels})\n",
    "        print(epoch, \"Accuracy:\", acc_batch)\n",
    "    \n",
    "    # save the trained model\n",
    "    save_path = tf.train.Saver().save(sess, \"./trained_mnist_ann.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "KWgEK5QhGbcM",
    "outputId": "ae4ed496-944d-4a61-f47e-2a57807ea24d"
   },
   "outputs": [],
   "source": [
    "# random one digit\n",
    "rnd_id = np.random.randint(0, len(digits))\n",
    "\n",
    "# show the digit\n",
    "plt.figure()\n",
    "plt.imshow(digits[rnd_id])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "# load the trained model and use to predict\n",
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess, \"./trained_mnist_ann.ckpt\")\n",
    "    Z = logits.eval(feed_dict={X: t_digits[rnd_id].reshape(1, 28*28)})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "print(\"Predicted class: \", y_pred)\n",
    "print(\"Actual class: \", labels[rnd_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_VdtRPQGbcQ"
   },
   "source": [
    "Rerun the cell above multiple times to see how accurate our trained model is. You should be able to see that the resulted accuracy is very low and our training is slightly time-consuming.\n",
    "\n",
    "**_Exercise 4_**\n",
    "Modify and tune the neural net such that the training time is reduced but the accuracy is still acceptably high. You should try the following:\n",
    "- Change the structure of the network by adding/removing a hidden layer or increasing/reducing number of neurons.\n",
    "- Change the activation function of the hidden layers.\n",
    "- Choose different optimisation algorithms such as tf.train.MomentumIptimizer(), tf.train.RMSPropOptimizer, and tf.train.AdamOptimizer(). Don't forget to change the training parameters accordingly.\n",
    "\n",
    "Do you observe any effect on the accuracy during the tuning? What is the best model that you can achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z0GphNiGbcR"
   },
   "source": [
    "## Convolutional Neural Network (CNN) with TensorFlow\n",
    "\n",
    "We now move on to convolutional neural net (CNN). The idea behind this architecture originated from a study on the animal visual cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5FCXFfwGbcS"
   },
   "outputs": [],
   "source": [
    "# Load and use all digits in MNIST if you have directly jumped to this section\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "digits = np.concatenate((X_train, X_test))\n",
    "labels = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Pre-processing the data\n",
    "t_digits = digits.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "t_labels = labels.astype(np.int32)\n",
    "\n",
    "# MNIST's specification\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3ln9tAtGbcT"
   },
   "source": [
    "As usual, we begin with creating TensorFlow computation graph, a loss function, and an optimiser for training the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9ofQrIQGbcU"
   },
   "outputs": [],
   "source": [
    "reset_graph() # as we defined in the beginning of this notebook\n",
    "\n",
    "# Create TensorFlow's placeholders for digits and labels\n",
    "X = tf.placeholder(tf.float32, shape=[None, height * width], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "# Construct 2D convolutional layers\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=20, kernel_size=3, strides=1, \n",
    "                         padding=\"SAME\", activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=40, kernel_size=3, strides=2, \n",
    "                         padding=\"SAME\", activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# Create a max pooling layer\n",
    "pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "pool3_flat = tf.reshape(pool3, shape=[-1, 40 * 7 * 7])\n",
    "\n",
    "# Followed by layer of fully-connected neurons\n",
    "fc1 = tf.layers.dense(pool3_flat, 50, activation=tf.nn.relu, name=\"fc1\")\n",
    "logits = tf.layers.dense(fc1, 10, name=\"output\")\n",
    "\n",
    "# Use mean softmax cross entropy as a loss function\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "# Use Adam Optimiser to train CNN \n",
    "training_op = tf.train.AdamOptimizer().minimize(loss, \n",
    "                                                aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "# (Change the aggregation_method to tf.AggregationMethod.EXPERIMENTAL_TREE or DEFAULT if it doesn't work)\n",
    "\n",
    "# Define accuracy measure\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BviuGKPiGbcX"
   },
   "source": [
    "Then, we proceed to executing the TensorFlow computation graph, which will train our CNN.\n",
    "\n",
    "<font color=\"red\">**_Note that training a CNN is generally time- and memory-consuming. It is very likely that your PC will either be slow down or frozen. If this is the case, click the stop button above, wait for a couple of minutes, restart your Python kernel, and jump down to the exercise below._**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mGMd-kNGbcX"
   },
   "outputs": [],
   "source": [
    "# Define a function to make training batches\n",
    "# This is useful when your PC doesn't have much memory\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Train the CNN batch by batch\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(10):\n",
    "        for X_batch, y_batch in shuffle_batch(t_digits, t_labels, 50): \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: t_digits, y: t_labels})\n",
    "        print(epoch, \"Accuracy:\", acc_batch)\n",
    "    \n",
    "    # save the trained model\n",
    "    save_path = tf.train.Saver().save(sess, \"./trained_mnist_cnn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-plw4uePGbca"
   },
   "outputs": [],
   "source": [
    "# random one digit for test CNN's prediction\n",
    "rnd_id = np.random.randint(0, len(digits))\n",
    "\n",
    "# visualise the digit\n",
    "plt.figure()\n",
    "plt.imshow(digits[rnd_id])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "# load the trained model and use to predict\n",
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess, \"./trained_mnist_cnn.ckpt\")\n",
    "    Z = logits.eval(feed_dict={X: t_digits[rnd_id].reshape(1, 28*28)})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "\n",
    "print(\"Predicted class: \", y_pred)\n",
    "print(\"Actual class: \", labels[rnd_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eEqnbgeGbcc"
   },
   "source": [
    "**_Exercise 5_**\n",
    "1. Visualise and/or draw on your paper this convolutional neural net to figure out its current structure.\n",
    "2. Tune the model such that the accuracy is acceptably good, the required memory is low, and the training time is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIzCfGQWGbcc"
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "'With 4 parameters I can fit an elephant and with 5 I can make him wiggle his trunk.' John von Neumann, _cited by Enrico Fermi in Nature 427_\n",
    "\n",
    "Do not forget that an overfitted model will not perform well in the real world. It is therefore important for you to know how to prevent this issue with neural networks in general.\n",
    "\n",
    "**_Exercise 6_**\n",
    "1. Recall the characteristic of overfitted models with respect to their performance on the training and test sets.\n",
    "2. Restore this notebook back to its original state and then modify the code above to partition the MNIST dataset into training set and test set.\n",
    "3. Further modify the training phase of deep net and/or CNN to use only the training set and evaluate accuracy or loss on both datasets.\n",
    "4. On deep net and/or CNN for MNIST above, implement one or a combination of the regularisation techniques listed below. Observe any difference or change in performance during training:\n",
    "\n",
    "   4.1. Early stopping, where you stop training your model if there is no further significant improvement of performance on your test set. (_Hint_: regularly check the performance on both sets and always store the best model)\n",
    "   \n",
    "   4.2. $l_1$ or $l_2$ regularisation, by correctly specifying TensorFlow parameters. (_Hint_: Look for 'kernel_regularizer' in the online documentation)\n",
    "   \n",
    "   4.3. Dropout, where each neuron has a probability of being turned off at each epoch in training phase (_Hint_: apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/dropout\">tf.layers.dropout()</a> to the input layer and/or any hidden layer's output, but NOT the output of the output layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aGfVCIzGbcc"
   },
   "source": [
    "## Sidenote\n",
    "There are many high level APIs that you can use to quickly create and deploy Machine Learning prototypes. They are very useful but it is difficult to make non-standard changes to their implementation of Machine Learning models. If you are interested, have a look on the following:\n",
    "- Estimators: <a href=\"https://www.tensorflow.org/guide/estimators\">https://www.tensorflow.org/guide/estimators</a>\n",
    "- Keras: <a href=\"https://www.tensorflow.org/guide/keras\">https://www.tensorflow.org/guide/keras</a>\n",
    "- Eager execution: <a href=\"https://www.tensorflow.org/guide/eager\">https://www.tensorflow.org/guide/eager</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic5qgx9cGbce"
   },
   "source": [
    "## Reference\n",
    "Aurélien Géron, _Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
